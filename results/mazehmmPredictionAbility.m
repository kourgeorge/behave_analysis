function mazehmmPredictionAbility()
%MAZEHMMPREDICTIONABILITY Summary of this function goes here
%   Detailed explanation goes here

%The goal of this test is to be comparable to the action prediction test
%that is perfomed on the real word data.
%In that test we try to predict the next action based on the obaservations
%of the last sequence of actions. The training is done on a different
%sequence generated by the same theta. However here since the data is synthetic, the ground truth is
%available. Nevertheless, we will use the exact same methodology olso for
%the the curresnt state estimation.


%The following is the descrition of this test flow.
%Create observations sequence given the true model.
%Estimate the model parameters or use random parameters.(*)
%Predict the next action
%Calculate the accuracy of predicting underlying strategy at time $n$ given
%the true undelying strategy at time $n-1$ and action at at trial $n$

%(*) Estimating the model parameters is done in several ways:
%1. random transition matrices (but true emmission matrices)
%2. estimation based on shuffled sequence
%3. estimation based on the training sequence

%Dimensions: 1. different training lengths. 2.different true model parameters.
%3. Drifting model parameters??? We still not sure how to do this.


sequence_lengths = floor(linspace(40,600,20));
repetitions  = 20;
inference_repretitions = 20;
inference_type = 'viterbi_prefix';
gt_param_type = 'gt';

action_prediction_accuracy_per_length = [];
state_prediction_accuracy_per_length = [];
for sequence_length = sequence_lengths
    action_prediction_accuracy_inorder = [];
    action_prediction_accuracy_shuffled = [];
    action_prediction_accuracy_gt = [];
    action_prediction_accuracy_random = [];
    
    state_prediction_accuracy_inorder = [];
    state_prediction_accuracy_shuffled = [];
    state_prediction_accuracy_gt = [];
    state_prediction_accuracy_random = [];
    
    for i=1:repetitions
       
        [train_envtype,train_actions,~,train_rewards] = generate_synthetic_sequence(sequence_length, gt_param_type);
        
        theta = estimate_theta(train_envtype,train_actions,train_rewards);
        
        permutation = randperm(length(train_actions));
        theta_s = estimate_theta(train_envtype(permutation),train_actions(permutation),train_rewards(permutation));
        
        theta_gt = getModelParameters( 0.01 , gt_param_type);
        
        theta_r = getModelParameters( 0.01 , 'random' );
        
        for k=1:inference_repretitions
            
            [test_envtype,test_actions,test_states,test_rewards]= generate_synthetic_sequence(100, gt_param_type);
            
            [action_accuracy, state_accuracy] = next_action_state_prediction_accuracy(test_envtype,test_actions, test_states, test_rewards, theta, inference_type);
            [s_action_accuracy, s_state_accuracy] = next_action_state_prediction_accuracy(test_envtype,test_actions, test_states, test_rewards, theta_s, inference_type);
            [gt_action_accuracy, gt_state_accuracy] = next_action_state_prediction_accuracy(test_envtype,test_actions, test_states, test_rewards, theta_gt, inference_type);
            [r_action_accuracy, r_state_accuracy] = next_action_state_prediction_accuracy(test_envtype,test_actions, test_states, test_rewards, theta_r, inference_type);
            
            action_prediction_accuracy_inorder = [action_prediction_accuracy_inorder, action_accuracy];
            action_prediction_accuracy_shuffled = [action_prediction_accuracy_shuffled, s_action_accuracy];
            action_prediction_accuracy_gt = [action_prediction_accuracy_gt, gt_action_accuracy];
            action_prediction_accuracy_random = [action_prediction_accuracy_random, r_action_accuracy];
            
            state_prediction_accuracy_inorder = [state_prediction_accuracy_inorder, state_accuracy];
            state_prediction_accuracy_shuffled = [state_prediction_accuracy_shuffled, s_state_accuracy];
            state_prediction_accuracy_gt = [state_prediction_accuracy_gt, gt_state_accuracy];
            state_prediction_accuracy_random = [state_prediction_accuracy_random, r_state_accuracy];
        end
    end
    action_prediction_accuracy_per_length = [action_prediction_accuracy_per_length;...
        mean(action_prediction_accuracy_random), mean(action_prediction_accuracy_shuffled), [mean(action_prediction_accuracy_inorder), mean(action_prediction_accuracy_gt)]];
    state_prediction_accuracy_per_length = [state_prediction_accuracy_per_length;...
        mean(state_prediction_accuracy_random), mean(state_prediction_accuracy_shuffled), [mean(state_prediction_accuracy_inorder), mean(state_prediction_accuracy_gt)]];
    
end

subplot(1,2,1)
plot(sequence_lengths, action_prediction_accuracy_per_length, 'LineWidth',2)
legend('random', 'shuffled', 'inorder', 'true parameters' )
ylim([0.4,0.9])
xlabel('Sequence length')
ylabel('Action prediction accuracy')

subplot(1,2,2)
plot(sequence_lengths, state_prediction_accuracy_per_length, 'LineWidth',2)
legend('random', 'shuffled', 'inorder', 'true parameters')
ylim([0.2,0.7])
xlabel('Sequence length')
ylabel('State prediction accuracy')
end


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Helper Functions %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function theta = estimate_theta(envtype, action, reward)
eps = 0.01;
theta_guess = getModelParameters( eps, 'uniform');
[theta.trR, theta.trNR, theta.eH, theta.eT] = mazehmmtrain(action, envtype, reward, ...
    theta_guess.trR, theta_guess.trNR, theta_guess.eH, theta_guess.eT,...
    'VERBOSE', false, 'maxiterations', 500, 'TOLERANCE',1e-4);
end


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [envtype,seq,states,rewards] = generate_synthetic_sequence(num_trials, gt_param_type)

theta = getModelParameters( 0.01 , gt_param_type );
[envtype,seq,states,rewards] = mazehmmgenerate(num_trials, theta.trR, theta.trNR, theta.eH, theta.eT, 0.5, [1 0; 1 0] );

end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function estimatedstates = estimate_hidden_states(actions,envtype,rewards,theta)
estimatedstates = mazehmmviterbi(actions, envtype, rewards, ...
    theta.trR, theta.trNR, theta.eH, theta.eT);
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [action_accuracy, state_accuracy] = next_action_state_prediction_accuracy(test_envtype,test_actions, test_states,test_rewards, theta, type)

if strcmp(type,'estimate_all')
    [action_accuracy, state_accuracy] = next_action_state_prediction_accuracy_v2(test_envtype,test_actions, test_states,test_rewards, theta);
    return;
end

if strcmp(type, 'viterbi_prefix')
    try
        estimated_states = estimate_hidden_states(test_actions(1:end-1), test_envtype(1:end-1), test_rewards(1:end-1), theta);
        last_state = estimated_states(end);
    catch exp
        warning(exp.message);
        action_accuracy=0;
        state_accuracy=0;
        return;
    end
end

if strcmp(type, 'gt_prefix')
    last_state = test_states(end-1);
end

last_reward = test_rewards(end-1);
next_envtype = test_envtype(end);
next_action_actual = test_actions(end);
next_state_actual = test_states(end);

[next_state_prediction, next_action_prediction] = ...
    predictNextStateandActionByModelParameters(theta, last_state, next_envtype, last_reward);

action_accuracy = next_action_actual ==next_action_prediction;
state_accuracy = next_state_actual == next_state_prediction;
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [action_accuracy, state_accuracy] = next_action_state_prediction_accuracy_v2(test_envtype,test_actions, test_states,test_rewards, theta)
%In this version we use the viterbi algorithm to estimate the state of the
%last trial and not as before where we use viterbi to estimate the one
%before last and select the max probability next state using the transition
%matrix.
try
    estimated_states = estimate_hidden_states(test_actions, test_envtype, test_rewards, theta);
    
catch exp
    warning(exp.message);
    action_accuracy=0;
    state_accuracy=0;
    return;
end

if (test_envtype(end)==1)
    emission = estimated_parameters.eh;
else
    emission = estimated_parameters.et;
end

[~,next_action_prediction] = max(emission(next_state,:));

next_state_prediction = estimated_states(end);
next_action_actual = test_actions(end);
next_state_actual = test_states(end);

action_accuracy = next_action_actual ==next_action_prediction;
state_accuracy = next_state_actual == next_state_prediction;
end